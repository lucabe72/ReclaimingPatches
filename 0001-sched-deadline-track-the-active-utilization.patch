From c10ff2355ecbd5330014587e9a7b9e25c6d0fb24 Mon Sep 17 00:00:00 2001
From: Luca Abeni <luca.abeni@unitn.it>
Date: Tue, 13 Oct 2015 13:13:09 +0200
Subject: [PATCH 1/6] sched/deadline: track the active utilization

Active utilization is defined as the total utilization of active
(TASK_RUNNING) tasks queued on all the runqueues of the root
domain. Hence, it is increased when a task wakes up and is decreased
when a task blocks.

Signed-off-by: Juri Lelli <juri.lelli@arm.com>
Signed-off-by: Luca Abeni <luca.abeni@unitn.it>
---
 kernel/sched/core.c     |  3 +++
 kernel/sched/deadline.c | 68 ++++++++++++++++++++++++++++++++++++++++++++++---
 kernel/sched/sched.h    |  9 +++++++
 3 files changed, 77 insertions(+), 3 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index c56fb57..250eb07 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -5885,6 +5885,9 @@ static int init_rootdomain(struct root_domain *rd)
 
 	if (cpupri_init(&rd->cpupri) != 0)
 		goto free_rto_mask;
+
+	raw_spin_lock_init(&rd->running_bw_lock);
+
 	return 0;
 
 free_rto_mask:
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index 70ef2b1..199b36f 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -43,6 +43,34 @@ static inline int on_dl_rq(struct sched_dl_entity *dl_se)
 	return !RB_EMPTY_NODE(&dl_se->rb_node);
 }
 
+static inline
+void add_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+	u64 old;
+
+	raw_spin_lock(&rq->rd->running_bw_lock);
+	old = rq->rd->running_bw;
+	rq->rd->running_bw += dl_se->dl_bw;
+	SCHED_WARN_ON(rq->rd->running_bw < old); /* overflow */
+	raw_spin_unlock(&rq->rd->running_bw_lock);
+}
+
+static inline
+void sub_running_bw(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)
+{
+	struct rq *rq = rq_of_dl_rq(dl_rq);
+	u64 old;
+
+	raw_spin_lock(&rq->rd->running_bw_lock);
+	old = rq->rd->running_bw;
+	rq->rd->running_bw -= dl_se->dl_bw;
+	SCHED_WARN_ON(rq->rd->running_bw > old); /* underflow */
+	if (rq->rd->running_bw > old)
+		rq->rd->running_bw = 0;
+	raw_spin_unlock(&rq->rd->running_bw_lock);
+}
+
 static inline int is_leftmost(struct task_struct *p, struct dl_rq *dl_rq)
 {
 	struct sched_dl_entity *dl_se = &p->dl;
@@ -909,8 +937,12 @@ enqueue_dl_entity(struct sched_dl_entity *dl_se,
 	 * parameters of the task might need updating. Otherwise,
 	 * we want a replenishment of its runtime.
 	 */
-	if (flags & ENQUEUE_WAKEUP)
+	if (flags & ENQUEUE_WAKEUP) {
+		struct dl_rq *dl_rq = dl_rq_of_se(dl_se);
+
+		add_running_bw(dl_se, dl_rq);
 		update_dl_entity(dl_se, pi_se);
+	}
 	else if (flags & ENQUEUE_REPLENISH)
 		replenish_dl_entity(dl_se, pi_se);
 
@@ -947,14 +979,25 @@ static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 		return;
 	}
 
+	if (p->on_rq == TASK_ON_RQ_MIGRATING)
+		add_running_bw(&p->dl, &rq->dl);
+
 	/*
-	 * If p is throttled, we do nothing. In fact, if it exhausted
+	 * If p is throttled, we do not enqueue it. In fact, if it exhausted
 	 * its budget it needs a replenishment and, since it now is on
 	 * its rq, the bandwidth timer callback (which clearly has not
 	 * run yet) will take care of this.
+	 * However, the active utilization does not depend on the fact
+	 * that the task is on the runqueue or not (but depends on the
+	 * task's state - in GRUB parlance, "inactive" vs "active contending").
+	 * In other words, even if a task is throttled its utilization must
+	 * be counted in the active utilization; hence, we need to call
+	 * add_running_bw().
 	 */
-	if (p->dl.dl_throttled && !(flags & ENQUEUE_REPLENISH))
+	if (p->dl.dl_throttled && !(flags & ENQUEUE_REPLENISH)) {
+		add_running_bw(&p->dl, &rq->dl);
 		return;
+	}
 
 	enqueue_dl_entity(&p->dl, pi_se, flags);
 
@@ -972,6 +1015,21 @@ static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)
 {
 	update_curr_dl(rq);
 	__dequeue_task_dl(rq, p, flags);
+
+	if (p->on_rq == TASK_ON_RQ_MIGRATING)
+		sub_running_bw(&p->dl, &rq->dl);
+
+	/*
+	 * This check allows to start the inactive timer (or to immediately
+	 * decrease the active utilization, if needed) in two cases:
+	 * when the task blocks and when it is terminating
+	 * (p->state == TASK_DEAD). We can handle the two cases in the same
+	 * way, because from GRUB's point of view the same thing is happening
+	 * (the task moves from "active contending" to "active non contending"
+	 * or "inactive")
+	 */
+	if (flags & DEQUEUE_SLEEP)
+		sub_running_bw(&p->dl, &rq->dl);
 }
 
 /*
@@ -1695,6 +1753,9 @@ static void switched_from_dl(struct rq *rq, struct task_struct *p)
 	if (!start_dl_timer(p))
 		__dl_clear_params(p);
 
+	if (task_on_rq_queued(p))
+		sub_running_bw(&p->dl, &rq->dl);
+
 	/*
 	 * Since this might be the only -deadline task on the rq,
 	 * this is the right place to try to pull some other one
@@ -1712,6 +1773,7 @@ static void switched_from_dl(struct rq *rq, struct task_struct *p)
  */
 static void switched_to_dl(struct rq *rq, struct task_struct *p)
 {
+	add_running_bw(&p->dl, &rq->dl);
 
 	/* If p is not queued we will update its parameters at next wakeup. */
 	if (!task_on_rq_queued(p))
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7b34c78..470890e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -580,6 +580,15 @@ struct root_domain {
 	struct cpupri cpupri;
 
 	unsigned long max_cpu_capacity;
+
+	/*
+	 * "Active utilization" for this root domain (global active
+	 * utilization): increased when a task wakes up (becomes
+	 * TASK_RUNNING) and decreased when a
+	 * task blocks
+	 */
+	u32 running_bw;
+	raw_spinlock_t running_bw_lock;
 };
 
 extern struct root_domain def_root_domain;
-- 
2.7.4

